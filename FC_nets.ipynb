{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/8Dis-like/UCLALearning/blob/main/FC_nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNkrpu48ftnZ"
      },
      "source": [
        "# Fully connected networks\n",
        "\n",
        "In the previous notebook, you implemented a simple two-layer neural network class.  However, this class is not modular.  If you wanted to change the number of layers, you would need to write a new loss and gradient function.  If you wanted to optimize the network with different optimizers, you'd need to write new training functions.  If you wanted to incorporate regularizations, you'd have to modify the loss and gradient function.  \n",
        "\n",
        "Instead of having to modify functions each time, for the rest of the class, we'll work in a more modular framework where we define forward and backward layers that calculate losses and gradients respectively.  Since the forward and backward layers share intermediate values that are useful for calculating both the loss and the gradient, we'll also have these function return \"caches\" which store useful intermediate values.\n",
        "\n",
        "The goal is that through this modular design, we can build different sized neural networks for various applications.\n",
        "\n",
        "In this HW #3, we'll define the basic architecture, and in HW #4, we'll build on this framework to implement different optimizers and regularizations (like BatchNorm and Dropout).\n",
        "\n",
        "CS231n has built a solid API for building these modular frameworks and training them, and we will use their very well implemented framework as opposed to \"reinventing the wheel.\"  This includes using their Solver, various utility functions, and their layer structure.  This also includes nndl.fc_net, nndl.layers, and nndl.layer_utils.  As in prior assignments, we thank Serena Yeung & Justin Johnson for permission to use code written for the CS 231n class (cs231n.stanford.edu).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wggcbSzwftna"
      },
      "source": [
        "## Modular layers\n",
        "\n",
        "This notebook will build modular layers in the following manner.  First, there will be a forward pass for a given layer with inputs (`x`) and return the output of that layer (`out`) as well as cached variables (`cache`) that will be used to calculate the gradient in the backward pass.\n",
        "\n",
        "```python\n",
        "def layer_forward(x, w):\n",
        "  \"\"\" Receive inputs x and weights w \"\"\"\n",
        "  # Do some computations ...\n",
        "  z = # ... some intermediate value\n",
        "  # Do some more computations ...\n",
        "  out = # the output\n",
        "   \n",
        "  cache = (x, w, z, out) # Values we need to compute gradients\n",
        "   \n",
        "  return out, cache\n",
        "```\n",
        "\n",
        "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
        "\n",
        "```python\n",
        "def layer_backward(dout, cache):\n",
        "  \"\"\"\n",
        "  Receive derivative of loss with respect to outputs and cache,\n",
        "  and compute derivative with respect to inputs.\n",
        "  \"\"\"\n",
        "  # Unpack cache values\n",
        "  x, w, z, out = cache\n",
        "  \n",
        "  # Use values in cache to compute derivatives\n",
        "  dx = # Derivative of loss with respect to x\n",
        "  dw = # Derivative of loss with respect to w\n",
        "  \n",
        "  return dx, dw\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTbgwk2Kftnb",
        "outputId": "68654f76-5f19-4d06-e113-f132d40aaa34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "## Import and setups\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nndl.fc_net import *\n",
        "from cs231n.data_utils import get_CIFAR10_data\n",
        "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from cs231n.solver import Solver\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "  \"\"\" returns relative error \"\"\"\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoBZWmnTftnc",
        "outputId": "5a14824d-74e8-4971-e234-a053575dd862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (49000, 3, 32, 32) \n",
            "y_train: (49000,) \n",
            "X_val: (1000, 3, 32, 32) \n",
            "y_val: (1000,) \n",
            "X_test: (1000, 3, 32, 32) \n",
            "y_test: (1000,) \n"
          ]
        }
      ],
      "source": [
        "# Load the (preprocessed) CIFAR10 data.\n",
        "\n",
        "data = get_CIFAR10_data()\n",
        "for k in data.keys():\n",
        "  print('{}: {} '.format(k, data[k].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s61yJF8rftnc"
      },
      "source": [
        "## Linear layers\n",
        "\n",
        "In this section, we'll implement the forward and backward pass for the linear layers.\n",
        "\n",
        "The linear layer forward pass is the function `affine_forward` in `nndl/layers.py` and the backward pass is `affine_backward`.\n",
        "\n",
        "After you have implemented these, test your implementation by running the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o26cjmHftnc"
      },
      "source": [
        "### Affine layer forward pass\n",
        "\n",
        "Implement `affine_forward` and then test your code by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYlHYjWftnc",
        "outputId": "f3845358-8864-4946-b3f9-24ab8fd00e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing affine_forward function:\n",
            "difference: 9.769849468192957e-10\n"
          ]
        }
      ],
      "source": [
        "# Test the affine_forward function\n",
        "\n",
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape)\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
        "\n",
        "out, _ = affine_forward(x, w, b)\n",
        "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
        "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
        "\n",
        "# Compare your output with ours. The error should be around 1e-9.\n",
        "print('Testing affine_forward function:')\n",
        "print('difference: {}'.format(rel_error(out, correct_out)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAABHXODftnd"
      },
      "source": [
        "### Affine layer backward pass\n",
        "\n",
        "Implement `affine_backward` and then test your code by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Eprxq2_ftnd",
        "outputId": "d757d8a9-3983-4ccb-ef3b-11b4a4500201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing affine_backward function:\n",
            "dx error: 5.0304338658412205e-09\n",
            "dw error: 3.3786027041256256e-10\n",
            "db error: 3.2766516476600243e-12\n"
          ]
        }
      ],
      "source": [
        "# Test the affine_backward function\n",
        "\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "_, cache = affine_forward(x, w, b)\n",
        "dx, dw, db = affine_backward(dout, cache)\n",
        "\n",
        "# The error should be around 1e-10\n",
        "print('Testing affine_backward function:')\n",
        "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
        "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
        "print('db error: {}'.format(rel_error(db_num, db)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGoUrI7Zftnd"
      },
      "source": [
        "## Activation layers\n",
        "\n",
        "In this section you'll implement the ReLU activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzvOSJdBftne"
      },
      "source": [
        "### ReLU forward pass\n",
        "\n",
        "Implement the `relu_forward` function in `nndl/layers.py` and then test your code by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wa86O3uftne",
        "outputId": "4d8c8552-dc4c-4fff-d711-add4b90620ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing relu_forward function:\n",
            "difference: 4.999999798022158e-08\n"
          ]
        }
      ],
      "source": [
        "# Test the relu_forward function\n",
        "\n",
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "# Compare your output with ours. The error should be around 1e-8\n",
        "print('Testing relu_forward function:')\n",
        "print('difference: {}'.format(rel_error(out, correct_out)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DT2pVH1ftne"
      },
      "source": [
        "### ReLU backward pass\n",
        "Implement the `relu_backward` function in `nndl/layers.py` and then test your code by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG1yLo1Jftne",
        "outputId": "5b494a24-b5b9-44c3-d5db-bd5d88c87d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing relu_backward function:\n",
            "dx error: 3.2755982712853154e-12\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "\n",
        "# The error should be around 1e-12\n",
        "print('Testing relu_backward function:')\n",
        "print('dx error: {}'.format(rel_error(dx_num, dx)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKCyp8dSftne"
      },
      "source": [
        "## Combining the affine and ReLU layers\n",
        "\n",
        "Often times, an affine layer will be followed by a ReLU layer. So let's make one that puts them together.  Layers that are combined are stored in `nndl/layer_utils.py`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm7FifqOftne"
      },
      "source": [
        "### Affine-ReLU layers\n",
        "We've implemented `affine_relu_forward()` and `affine_relu_backward` in ``nndl/layer_utils.py``.  Take a look at them to make sure you understand what's going on.  Then run the following cell to ensure its implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Mgv-Ttftne",
        "outputId": "56b1cf67-e651-4536-9b7e-0583e3f6d3eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing affine_relu_forward and affine_relu_backward:\n",
            "dx error: 5.078537698879339e-11\n",
            "dw error: 2.189944703618186e-09\n",
            "db error: 6.880175915883598e-12\n"
          ]
        }
      ],
      "source": [
        "from nndl.layer_utils import affine_relu_forward, affine_relu_backward\n",
        "\n",
        "x = np.random.randn(2, 3, 4)\n",
        "w = np.random.randn(12, 10)\n",
        "b = np.random.randn(10)\n",
        "dout = np.random.randn(2, 10)\n",
        "\n",
        "out, cache = affine_relu_forward(x, w, b)\n",
        "dx, dw, db = affine_relu_backward(dout, cache)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "print('Testing affine_relu_forward and affine_relu_backward:')\n",
        "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
        "print('dw error: {}'.format(rel_error(dw_num, dw)))\n",
        "print('db error: {}'.format(rel_error(db_num, db)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP9ZfEBKftnf"
      },
      "source": [
        "## Softmax and SVM losses\n",
        "\n",
        "You've already implemented these, so we have written these in `layers.py`.  The following code will ensure they are working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNlzfrV2ftnf",
        "outputId": "1198f2e2-c688-4b93-91e9-797559f2776a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing svm_loss:\n",
            "loss: 9.000133705498998\n",
            "dx error: 8.182894472887002e-10\n",
            "\n",
            "Testing softmax_loss:\n",
            "loss: 2.302598890306366\n",
            "dx error: 7.938863722250494e-09\n"
          ]
        }
      ],
      "source": [
        "num_classes, num_inputs = 10, 50\n",
        "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
        "y = np.random.randint(num_classes, size=num_inputs)\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = svm_loss(x, y)\n",
        "\n",
        "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
        "print('Testing svm_loss:')\n",
        "print('loss: {}'.format(loss))\n",
        "print('dx error: {}'.format(rel_error(dx_num, dx)))\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = softmax_loss(x, y)\n",
        "\n",
        "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
        "print('\\nTesting softmax_loss:')\n",
        "print('loss: {}'.format(loss))\n",
        "print('dx error: {}'.format(rel_error(dx_num, dx)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA8t9XMiftnf"
      },
      "source": [
        "## Implementation of a two-layer NN\n",
        "\n",
        "In `nndl/fc_net.py`, implement the class `TwoLayerNet` which uses the layers you made here.  When you have finished, the following cell will test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFjAWejsftnf",
        "outputId": "8351d46a-fe82-4f81-9618-6660276310f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing initialization ... \n",
            "Testing test-time forward pass ... \n",
            "Testing training loss (no regularization)\n",
            "Running numeric gradient check with reg = 0.0\n",
            "W1 relative error: 1.8336562786695002e-08\n",
            "W2 relative error: 3.201560569143183e-10\n",
            "b1 relative error: 9.828315204644842e-09\n",
            "b2 relative error: 4.329134954569865e-10\n",
            "Running numeric gradient check with reg = 0.7\n",
            "W1 relative error: 2.5279152310200606e-07\n",
            "W2 relative error: 7.976652806155026e-08\n",
            "b1 relative error: 1.564679947504764e-08\n",
            "b2 relative error: 9.089617896905665e-10\n"
          ]
        }
      ],
      "source": [
        "N, D, H, C = 3, 5, 50, 7\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=N)\n",
        "\n",
        "std = 1e-2\n",
        "model = TwoLayerNet(input_dim=D, hidden_dims=H, num_classes=C, weight_scale=std)\n",
        "\n",
        "print('Testing initialization ... ')\n",
        "W1_std = abs(model.params['W1'].std() - std)\n",
        "b1 = model.params['b1']\n",
        "W2_std = abs(model.params['W2'].std() - std)\n",
        "b2 = model.params['b2']\n",
        "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
        "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
        "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
        "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
        "\n",
        "print('Testing test-time forward pass ... ')\n",
        "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
        "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
        "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
        "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
        "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
        "scores = model.loss(X)\n",
        "correct_scores = np.asarray(\n",
        "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
        "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
        "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
        "scores_diff = np.abs(scores - correct_scores).sum()\n",
        "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
        "\n",
        "print('Testing training loss (no regularization)')\n",
        "y = np.asarray([0, 5, 1])\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 3.4702243556\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
        "\n",
        "model.reg = 1.0\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 26.5948426952\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
        "\n",
        "for reg in [0.0, 0.7]:\n",
        "  print('Running numeric gradient check with reg = {}'.format(reg))\n",
        "  model.reg = reg\n",
        "  loss, grads = model.loss(X, y)\n",
        "\n",
        "  for name in sorted(grads):\n",
        "    f = lambda _: model.loss(X, y)[0]\n",
        "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
        "    print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnspPS5wftnf"
      },
      "source": [
        "## Solver\n",
        "\n",
        "We will now use the cs231n Solver class to train these networks.  Familiarize yourself with the API in `cs231n/solver.py`.  After you have done so, declare an instance of a TwoLayerNet with 200 units and then train it with the Solver.  Choose parameters so that your validation accuracy is at least 40%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "B1EFXBmFftnf"
      },
      "outputs": [],
      "source": [
        "model = TwoLayerNet()\n",
        "solver = None\n",
        "\n",
        "# ================================================================ #\n",
        "# YOUR CODE HERE:\n",
        "#   Declare an instance of a TwoLayerNet and then train\n",
        "#   it with the Solver. Choose hyperparameters so that your validation\n",
        "#   accuracy is at least 40%.  We won't have you optimize this further\n",
        "#   since you did it in the previous notebook.\n",
        "#\n",
        "# ================================================================ #\n",
        "\n",
        "pass\n",
        "\n",
        "# ================================================================ #\n",
        "# END YOUR CODE HERE\n",
        "# ================================================================ #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRYev4Cjftng"
      },
      "outputs": [],
      "source": [
        "# Run this cell to visualize training loss and train / val accuracy\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(solver.train_acc_history, '-o', label='train')\n",
        "plt.plot(solver.val_acc_history, '-o', label='val')\n",
        "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E1OnbvGftng"
      },
      "source": [
        "## Multilayer Neural Network\n",
        "\n",
        "Now, we implement a multi-layer neural network.\n",
        "\n",
        "Read through the `FullyConnectedNet` class in the file `nndl/fc_net.py`.\n",
        "\n",
        "Implement the initialization, the forward pass, and the backward pass.  There will be lines for batchnorm and dropout layers and caches; ignore these all for now.  That'll be in assignment #4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaWZ8DTnftng",
        "outputId": "9a395a75-c201-4dbb-f2e4-2c37f1601958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running check with reg = 0\n",
            "Initial loss: 2.304146153180416\n",
            "W1 relative error: 2.6801358244466505e-06\n",
            "W2 relative error: 1.374956462320052e-06\n",
            "W3 relative error: 8.417665745014667e-08\n",
            "b1 relative error: 1.2085842991163667e-08\n",
            "b2 relative error: 2.4058024882489714e-08\n",
            "b3 relative error: 6.832746086246097e-11\n",
            "Running check with reg = 3.14\n",
            "Initial loss: 6.967494883062306\n",
            "W1 relative error: 7.294088428691085e-09\n",
            "W2 relative error: 3.836274335648032e-08\n",
            "W3 relative error: 1.8812532473286155e-08\n",
            "b1 relative error: 2.0170370768147783e-07\n",
            "b2 relative error: 8.379107094662078e-08\n",
            "b3 relative error: 3.7618771892752947e-10\n"
          ]
        }
      ],
      "source": [
        "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=(N,))\n",
        "\n",
        "for reg in [0, 3.14]:\n",
        "  print('Running check with reg = {}'.format(reg))\n",
        "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
        "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
        "\n",
        "  loss, grads = model.loss(X, y)\n",
        "  print('Initial loss: {}'.format(loss))\n",
        "\n",
        "  for name in sorted(grads):\n",
        "    f = lambda _: model.loss(X, y)[0]\n",
        "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
        "    print('{} relative error: {}'.format(name, rel_error(grad_num, grads[name])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEg1cwjMftng",
        "outputId": "43852202-6a38-452c-9de6-d96947432667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 40) loss: 2.345440\n",
            "(Epoch 0 / 20) train acc: 0.020000; val_acc: 0.100000\n",
            "(Epoch 1 / 20) train acc: 0.020000; val_acc: 0.100000\n",
            "(Epoch 2 / 20) train acc: 0.020000; val_acc: 0.101000\n",
            "(Epoch 3 / 20) train acc: 0.020000; val_acc: 0.101000\n",
            "(Epoch 4 / 20) train acc: 0.020000; val_acc: 0.100000\n",
            "(Epoch 5 / 20) train acc: 0.020000; val_acc: 0.099000\n",
            "(Iteration 11 / 40) loss: 2.339104\n",
            "(Epoch 6 / 20) train acc: 0.020000; val_acc: 0.099000\n",
            "(Epoch 7 / 20) train acc: 0.020000; val_acc: 0.097000\n",
            "(Epoch 8 / 20) train acc: 0.020000; val_acc: 0.097000\n",
            "(Epoch 9 / 20) train acc: 0.020000; val_acc: 0.097000\n",
            "(Epoch 10 / 20) train acc: 0.040000; val_acc: 0.097000\n",
            "(Iteration 21 / 40) loss: 2.285994\n",
            "(Epoch 11 / 20) train acc: 0.040000; val_acc: 0.098000\n",
            "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.098000\n",
            "(Epoch 13 / 20) train acc: 0.080000; val_acc: 0.096000\n",
            "(Epoch 14 / 20) train acc: 0.080000; val_acc: 0.098000\n",
            "(Epoch 15 / 20) train acc: 0.080000; val_acc: 0.099000\n",
            "(Iteration 31 / 40) loss: 2.314630\n",
            "(Epoch 16 / 20) train acc: 0.080000; val_acc: 0.100000\n",
            "(Epoch 17 / 20) train acc: 0.080000; val_acc: 0.100000\n",
            "(Epoch 18 / 20) train acc: 0.080000; val_acc: 0.100000\n",
            "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.101000\n",
            "(Epoch 20 / 20) train acc: 0.080000; val_acc: 0.099000\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Use the three layer neural network to overfit a small dataset.\n",
        "\n",
        "num_train = 50\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "\n",
        "#### !!!!!!\n",
        "# Play around with the weight_scale and learning_rate so that you can overfit a small dataset.\n",
        "# Your training accuracy should be 1.0 to receive full credit on this part.\n",
        "weight_scale = 1e-2\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model = FullyConnectedNet([100, 100],\n",
        "              weight_scale=weight_scale, dtype=np.float64)\n",
        "solver = Solver(model, small_data,\n",
        "                print_every=10, num_epochs=20, batch_size=25,\n",
        "                update_rule='sgd',\n",
        "                optim_config={\n",
        "                  'learning_rate': learning_rate,\n",
        "                }\n",
        "         )\n",
        "solver.train()\n",
        "\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.title('Training loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}